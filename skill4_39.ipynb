{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name = Anubhav Kumar Tiwary\n",
    "# PRN: 2301208039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.11.4)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: pdf2image in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: python-pptx in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from pdfplumber) (10.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-pptx) (3.2.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-pptx) (5.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-pptx) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.7.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber PyPDF2 pdf2image pandas python-pptx beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ghostscript\n",
      "  Downloading ghostscript-0.7-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: setuptools>=38.6.0 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from ghostscript) (73.0.1)\n",
      "Downloading ghostscript-0.7-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: ghostscript\n",
      "Successfully installed ghostscript-0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install ghostscript\n",
    "import ghostscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF for PDF text extraction\n",
    "import pdf2image\n",
    "import camelot\n",
    "import pandas as pd\n",
    "from pptx import Presentation\n",
    "from pptx.enum.shapes import MSO_SHAPE_TYPE\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Directory to store extracted images\n",
    "IMAGE_DIR = \"extracted_images\"\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "def extract_from_pdf(pdf_path):\n",
    "    pdf_data = {\n",
    "        \"textual content\": \"\",\n",
    "        \"Images\": [],\n",
    "        \"Table Content\": [],\n",
    "        \"Links\": []\n",
    "    }\n",
    "    \n",
    "    # Open PDF file\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc[page_num]\n",
    "        \n",
    "        # Extract text and links\n",
    "        pdf_data[\"textual content\"] += page.get_text(\"text\")\n",
    "        pdf_data[\"Links\"].extend([link['uri'] for link in page.get_links() if link['uri']])\n",
    "        \n",
    "        # Extract images\n",
    "        for img_index, img in enumerate(page.get_images(full=True)):\n",
    "            base_image = doc.extract_image(img[0])\n",
    "            img_path = os.path.join(IMAGE_DIR, f\"pdf_image_{page_num}_{img_index}.png\")\n",
    "            with open(img_path, \"wb\") as img_file:\n",
    "                img_file.write(base_image[\"image\"])\n",
    "            pdf_data[\"Images\"].append(img_path)\n",
    "\n",
    "    # Extract tables\n",
    "    tables = camelot.read_pdf(pdf_path, pages='all')\n",
    "    for table in tables:\n",
    "        pdf_data[\"Table Content\"].append(table.df.to_dict())\n",
    "    \n",
    "    return pdf_data\n",
    "\n",
    "def extract_from_pptx(pptx_path):\n",
    "    ppt_data = {\n",
    "        \"textual content\": \"\",\n",
    "        \"Images\": [],\n",
    "        \"Table Content\": [],\n",
    "        \"Links\": []\n",
    "    }\n",
    "    \n",
    "    # Load PowerPoint\n",
    "    prs = Presentation(pptx_path)\n",
    "    for slide in prs.slides:\n",
    "        # Extract text\n",
    "        for shape in slide.shapes:\n",
    "            if shape.has_text_frame:\n",
    "                ppt_data[\"textual content\"] += shape.text + \"\\n\"\n",
    "            elif shape.shape_type == MSO_SHAPE_TYPE.PICTURE:\n",
    "                img_path = os.path.join(IMAGE_DIR, f\"ppt_image_{shape._element.get('id')}.png\")\n",
    "                with open(img_path, \"wb\") as img_file:\n",
    "                    img_file.write(shape.image.blob)\n",
    "                ppt_data[\"Images\"].append(img_path)\n",
    "        \n",
    "        # Extract hyperlinks\n",
    "        for shape in slide.shapes:\n",
    "            if shape.has_text_frame and shape.click_action.hyperlink.address:\n",
    "                ppt_data[\"Links\"].append(shape.click_action.hyperlink.address)\n",
    "\n",
    "        # Extract tables\n",
    "        for shape in slide.shapes:\n",
    "            if shape.has_table:\n",
    "                table_data = []\n",
    "                table = shape.table\n",
    "                for row in table.rows:\n",
    "                    row_data = [cell.text for cell in row.cells]\n",
    "                    table_data.append(row_data)\n",
    "                ppt_data[\"Table Content\"].append(table_data)\n",
    "\n",
    "    return ppt_data\n",
    "\n",
    "def extract_from_url(url):\n",
    "    url_data = {\n",
    "        \"textual content\": \"\",\n",
    "        \"Images\": [],\n",
    "        \"Table Content\": [],\n",
    "        \"Links\": []\n",
    "    }\n",
    "    \n",
    "    # Fetch and parse the webpage\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Extract text\n",
    "    url_data[\"textual content\"] = soup.get_text()\n",
    "    \n",
    "    # Extract images\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        img_url = img.get(\"src\")\n",
    "        if img_url:\n",
    "            url_data[\"Images\"].append(img_url)\n",
    "    \n",
    "    # Extract links\n",
    "    url_data[\"Links\"] = [a.get(\"href\") for a in soup.find_all(\"a\", href=True)]\n",
    "    \n",
    "    # Extract tables\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        table_data = []\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            row_data = [cell.get_text(strip=True) for cell in row.find_all([\"td\", \"th\"])]\n",
    "            table_data.append(row_data)\n",
    "        url_data[\"Table Content\"].append(table_data)\n",
    "\n",
    "    return url_data\n",
    "\n",
    "def save_to_csv(data, csv_filename):\n",
    "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"textual content\", \"Images\", \"Table Content\", \"Links\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerow({\n",
    "            \"textual content\": data[\"textual content\"],\n",
    "            \"Images\": \", \".join(data[\"Images\"]),\n",
    "            \"Table Content\": str(data[\"Table Content\"]),\n",
    "            \"Links\": \", \".join(data[\"Links\"])\n",
    "        })\n",
    "\n",
    "# Paths to the files\n",
    "pdf_path = \"Christopher Robert Evans.pdf\"\n",
    "pptx_path = \"Skill_4_PPT.pptx\"\n",
    "url = \"https://en.wikipedia.org/wiki/Elizabeth_Olsen\"\n",
    "\n",
    "# Extract and save data for each document type\n",
    "pdf_data = extract_from_pdf(pdf_path)\n",
    "save_to_csv(pdf_data, \"pdf_data.csv\")\n",
    "\n",
    "ppt_data = extract_from_pptx(pptx_path)\n",
    "save_to_csv(ppt_data, \"ppt_data.csv\")\n",
    "\n",
    "url_data = extract_from_url(url)\n",
    "save_to_csv(url_data, \"url_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
